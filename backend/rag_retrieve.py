"""
RAG Retrieval Module for Mistri.AI
Searches the vector database for relevant error codes using semantic similarity.
"""
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

import boto3
import numpy as np
import faiss

from backend.config import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RAGRetriever:
    """
    Handles semantic search over the error code vector database.
    
    This class loads the FAISS index and performs similarity search
    using embeddings generated by Amazon Bedrock Titan.
    
    TODO: Replace FAISS with Amazon Aurora pgvector for production.
    """
    
    def __init__(self):
        """Initialize the RAG retriever with Bedrock client and FAISS index."""
        try:
            # Initialize Bedrock Runtime client
            self.bedrock_runtime = boto3.client(
                service_name='bedrock-runtime',
                **config.get_boto3_session_kwargs()
            )
            logger.info(f"Initialized Bedrock client in region: {config.aws_region}")
        except Exception as e:
            logger.error(f"Failed to initialize Bedrock client: {e}")
            raise
        
        # Load FAISS index and metadata
        self.vector_store_path = Path(config.vector_store_path)
        self.index_file = self.vector_store_path / "faiss_index.bin"
        self.metadata_file = self.vector_store_path / "metadata.json"
        self.category_index_file = self.vector_store_path / "category_index.json"
        
        self._load_index()
    
    def _load_index(self):
        """
        Load FAISS index, metadata, and category index from disk.
        
        Raises:
            FileNotFoundError: If index files don't exist
        """
        if not self.index_file.exists():
            raise FileNotFoundError(
                f"FAISS index not found at {self.index_file}. "
                f"Please run rag_ingest.py first to create the index."
            )
        
        if not self.metadata_file.exists():
            raise FileNotFoundError(
                f"Metadata file not found at {self.metadata_file}. "
                f"Please run rag_ingest.py first."
            )
        
        try:
            # Load FAISS index
            # TODO: Replace with Amazon Aurora pgvector query
            # Example Aurora retrieval:
            # cursor.execute("""
            #     SELECT id, code, name, description,
            #            embedding <=> %s AS distance
            #     FROM error_embeddings
            #     WHERE category = %s
            #     ORDER BY distance
            #     LIMIT %s;
            # """, (query_embedding, intent_category, top_k))
            
            self.index = faiss.read_index(str(self.index_file))
            logger.info(f"Loaded FAISS index with {self.index.ntotal} vectors")
            
            # Load metadata
            with open(self.metadata_file, 'r') as f:
                self.metadata = json.load(f)
            logger.info(f"Loaded metadata for {len(self.metadata)} error codes")
            
            # Load category index (THE TREE STRUCTURE)
            if self.category_index_file.exists():
                with open(self.category_index_file, 'r') as f:
                    self.category_index = json.load(f)
                logger.info(f"Loaded category index: {[(k, len(v)) for k, v in self.category_index.items()]}")
            else:
                logger.warning("Category index not found. Filtered search will not be available.")
                self.category_index = {}
            
        except Exception as e:
            logger.error(f"Failed to load index: {e}")
            raise
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        Generate embedding for query text using Amazon Bedrock Titan.
        
        Args:
            text: Query text to embed
            
        Returns:
            numpy array of embedding vector (1536 dimensions)
            
        Raises:
            Exception: If Bedrock API call fails
        """
        try:
            # Prepare request body
            request_body = json.dumps({
                "inputText": text
            })
            
            # Invoke Bedrock model
            response = self.bedrock_runtime.invoke_model(
                modelId=config.bedrock_embed_model,
                contentType='application/json',
                accept='application/json',
                body=request_body
            )
            
            # Parse response
            response_body = json.loads(response['body'].read())
            embedding = np.array(response_body['embedding'], dtype=np.float32)
            
            return embedding
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def search_manual(
        self,
        query_text: str,
        query_image_bytes: Optional[bytes] = None,
        intent_category: Optional[str] = None,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        Search the error code database for relevant matches.
        
        **THE TREE SEARCHER**: Filters search by category based on user intent.
        
        Args:
            query_text: User's query text (e.g., "washing machine not draining")
            query_image_bytes: Optional image bytes for multimodal search (future)
            intent_category: Optional category filter ('ERROR_CODES', 'SCHEMATICS', 'GENERAL')
            top_k: Number of top results to return
            
        Returns:
            List of dictionaries containing matched error codes with metadata
            
        Example:
            >>> retriever = RAGRetriever()
            >>> # Search only error codes
            >>> results = retriever.search_manual("IE error", intent_category="ERROR_CODES")
            >>> print(results[0]['code'])  # 'IE'
            >>> 
            >>> # Search all categories
            >>> results = retriever.search_manual("washing machine problem")
        """
        try:
            # Generate query embedding
            logger.info(f"Searching for: {query_text}")
            if intent_category:
                logger.info(f"Filtering by category: {intent_category}")
            
            query_embedding = self.generate_embedding(query_text)
            
            # Reshape for FAISS (expects 2D array)
            query_vector = query_embedding.reshape(1, -1)
            
            # THE TREE LOGIC: Filter by category if specified
            if intent_category and intent_category in self.category_index:
                # Get valid IDs for this category
                valid_ids = self.category_index[intent_category]
                
                if not valid_ids:
                    logger.warning(f"No embeddings found for category: {intent_category}")
                    return []
                
                logger.info(f"Searching {len(valid_ids)} embeddings in category {intent_category}")
                
                # Create ID selector for filtered search
                id_selector = faiss.IDSelectorBatch(np.array(valid_ids, dtype=np.int64))
                
                # Create search parameters with ID filter
                params = faiss.SearchParametersIVF()
                params.sel = id_selector
                
                # Perform filtered search
                # Note: For IndexFlatL2, we need to manually filter results
                # since it doesn't support search parameters
                distances, indices = self.index.search(query_vector, min(top_k * 3, self.index.ntotal))
                
                # Filter results to only include valid category IDs
                filtered_results = []
                for dist, idx in zip(distances[0], indices[0]):
                    if idx in valid_ids:
                        filtered_results.append((dist, idx))
                        if len(filtered_results) >= top_k:
                            break
                
                distances = np.array([[r[0] for r in filtered_results]])
                indices = np.array([[r[1] for r in filtered_results]])
            else:
                # Search all embeddings (no filter)
                logger.info(f"Searching all {self.index.ntotal} embeddings")
                distances, indices = self.index.search(query_vector, top_k)
            
            # Prepare results
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(self.metadata):
                    result = self.metadata[idx].copy()
                    result['similarity_score'] = float(1 / (1 + distance))  # Convert distance to similarity
                    result['rank'] = i + 1
                    results.append(result)
            
            logger.info(f"Found {len(results)} matches")
            return results
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            raise


# Global retriever instance (lazy loaded)
_retriever: Optional[RAGRetriever] = None


def get_retriever() -> RAGRetriever:
    """
    Get or create the global RAGRetriever instance.
    
    Returns:
        RAGRetriever instance
    """
    global _retriever
    if _retriever is None:
        _retriever = RAGRetriever()
    return _retriever


def search_manual(
    query_text: str,
    query_image_bytes: Optional[bytes] = None,
    intent_category: Optional[str] = None,
    top_k: int = 3
) -> List[Dict[str, Any]]:
    """
    Convenience function for searching the manual database.
    
    Args:
        query_text: User's query text
        query_image_bytes: Optional image bytes
        intent_category: Optional category filter ('ERROR_CODES', 'SCHEMATICS', 'GENERAL')
        top_k: Number of results to return
        
    Returns:
        List of matched error codes
    """
    retriever = get_retriever()
    return retriever.search_manual(query_text, query_image_bytes, intent_category, top_k)
